<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Svicen</title>
    <link>https://hugmymind.github.io/posts/</link>
    <description>Recent content in Posts on Svicen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Oct 2023 16:09:33 +0800</lastBuildDate><atom:link href="https://hugmymind.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>VAE</title>
      <link>https://hugmymind.github.io/posts/vae/</link>
      <pubDate>Fri, 27 Oct 2023 16:09:33 +0800</pubDate>
      
      <guid>https://hugmymind.github.io/posts/vae/</guid>
      <description>从PCA到VAE 在第一部分中，我们将从讨论与降维有关的一些概念开始。特别是，我们将简要回顾主成分分析（PCA）和自动编码器，以显示这两种思想之间的相互关系。
什么是降维 在机器学习中，降维是减少描述某些数据的特征数量的过程。可以通过选择（仅保留一些现有特征）或通过提取（基于旧特征来减少数量的新特征）来进行此简化，并且在许多需要低维数据（数据可视化，数据存储，繁重的计算&amp;hellip;）。尽管存在许多不同的降维方法，但是我们可以设置一个与大多数（如果没有的话）方法相匹配的全局框架。
首先，我们称编码器为从“旧特征”表示中产生“新特征”表示（通过选择或提取）的过程，然后将逆过程解码。然后，降维可以解释为数据压缩，其中编码器压缩数据（从初始空间到编码空间，也称为潜在空间），而解码器将其解压缩。当然，根据初始数据分布，潜在空间尺寸和编码器定义，此压缩可能是有损的，这意味着一部分信息在编码过程中丢失，并且在解码时无法恢复。
降维方法的主要目的是在给定系列中找到最佳的编码器/解码器对。 换句话说，对于给定的一组可能的编码器和解码器，我们正在寻找在编码时保持信息量最大，因此在解码时具有最小重构误差的对。
主成分分析（PCA） PCA的想法是构建 $n_e$ 个新的独立特征，这些特征是 $n_d$ 个旧特征的线性组合，以便这些新特征所定义的子空间上的数据投影尽可能接近初始数据（就欧几里得而言） 距离）。 换句话说，PCA正在寻找初始空间的最佳线性子空间（通过新特征的正交描述），以使通过其在该子空间上的投影近似数据的误差尽可能小。
在我们的全局框架中进行转换，我们正在寻找 $n_e$ 族的 $n_d$ 矩阵（线性变换）中的编码器，其行是正交的（特征独立性），以及 $n_d$ 族D的相关解码器（$n_e$ 矩阵）。 可以证明，与协方差特征矩阵的 $n_e$ 个最大特征值（在范数上）相对应的unit特征向量是正交的（或可以这样选择），并定义维度 $n_e$ 的最佳子空间以最小的误差将数据投影到 近似。 因此，可以选择这些 $n_e$ 个特征向量作为我们的新特征，因此，降维问题可以表示为特征值/特征向量问题。 此外，还可以示出，在这种情况下，解码器矩阵是编码器矩阵的转置。
Autoencoders 自动编码器的总体思想非常简单，将编码器和解码器设置为神经网络，并使用迭代优化过程学习最佳的编码解码方案。因此，在每次迭代中，我们向自动编码器架构（编码器后跟解码器）提供一些数据，将编码解码输出与初始数据进行比较，并通过架构反向传播误差以更新网络的权重。
因此，直观地说，整个自动编码器架构（编码器+解码器）为数据创建了一个瓶颈（bottleneck），确保只有信息的主要结构化部分可以通过并被重建。看看我们的总体框架，所考虑的编码器系列 E 由编码器网络架构定义，所考虑的解码器系列 D 由解码器网络架构定义，并且通过梯度下降完成最小化重建误差的编码器和解码器的搜索通过这些网络的参数。
首先假设我们的编码器和解码器架构都只有一层，没有非线性（线性自动编码器）。这样的编码器和解码器是可以表示为矩阵的简单线性变换。在这种情况下，我们可以看到与 PCA 的明显联系，就像 PCA 一样，我们正在寻找最佳的线性子空间来投影数据，同时尽可能减少信息丢失。用 PCA 获得的编码和解码矩阵自然地定义了我们通过梯度下降所满意达到的解决方案之一，但我们应该概述这不是唯一的解决方案。事实上，可以选择多个基(basis)来描述相同的最佳子空间，因此，多个编码器/解码器对可以给出最佳重建误差。此外，对于线性自动编码器，与 PCA 相反，我们最终得到的新特征不必是独立的（神经网络中没有正交性约束）。
现在，我们假设编码器和解码器都是深度且非线性的。在这种情况下，架构越复杂，自动编码器就越能进行高维数降低，同时保持较低的重建损失。直观地说，如果我们的编码器和解码器有足够的自由度，我们可以将任何初始维度减少到 1。
然而，我们应该记住两件事。
在没有重建损失的情况下进行重要的降维通常会带来代价：潜在空间中缺乏可解释和可利用的结构（缺乏规律性）。 很多时候降维的最终目的不仅仅是降低数据的维数，而是降低维数的同时将数据结构信息的主要部分保留在减少的表示中。 Limitation 此时我们想到的一个自然问题是“自动编码器和内容生成之间的联系是什么？”，事实上，一旦自动编码器经过训练，我们就有了编码器和解码器，但仍然没有真正的方法来生成任何新内容。乍一看，我们可能会想，如果潜在空间足够规则（编码器在训练过程中“组织”得很好），我们可以从该潜在空间中随机取出一个点并将其解码以获得新的内容。然后，解码器的行为或多或少类似于生成对抗网络的生成器
但是，正如我们在上一节中讨论的那样，自动编码器的潜在空间的规则性是一个难点，这取决于初始空间中数据的分布，潜在空间的大小和编码器的体系结构。因此，很难（如果不是不可能）先验地确保编码器将以与我们刚刚描述的生成过程兼容的智能方式组织潜空间。
为了说明这一点，让我们考虑之前给出的示例，在该示例中，我们描述了一种强大的编码器和解码器，可以将任何N个初始训练数据放到实轴上（每个数据点都被编码为实数值），并且无需任何解码就可以解码重建损失。在这种情况下，自动编码器的高度自由度使得可以在没有信息损失的情况下进行编码和解码（尽管潜在空间的维数较低）会导致严重的过拟合，这意味着潜在空间的某些点将给出无意义的内容一旦解码。如果自愿选择此一维示例的极端性，我们可以注意到自动编码器潜在空间规则性的问题要比这普遍得多，应特别注意。
稍微想一下就会明白，**将数据编码到潜空间后，样本之间不连续，存在很多没有意义的潜向量。这是合理的，**因为在自动编码器的训练过程中，的确没有约束潜空间向量之间的关联：无论如何组织潜在空间，自动编码器的训练目标只是以尽可能少的损失进行编码和解码。因此，如果我们没有谨慎地定义编码器和解码器的网络结构，那么很自然的，在训练过程中，网络会利用任何过度拟合的可能性来尽可能地完成其任务……除非我们明确对其进行规范化！
因此，为了能够将自动编码器的解码器用于生成目的，我们必须确保潜在空间足够规则。获得这种规律性的一种可能的解决方案是在训练过程中引入显式正则化。因此，正如我们在本文的介绍中简要提到的，变分自动编码器可以定义为一种自动编码器，其训练经过正则化以避免过度拟合并确保潜在空间具有支持生成过程的良好属性。
Variational Autoencoders 就像标准自动编码器一样，变分自动编码器是一种由编码器和解码器组成的架构，经过训练以最小化编码解码数据与初始数据之间的重构误差。然而，为了引入潜在空间的一些正则化，我们对编码解码过程进行了轻微的修改：我们不是将输入编码为单个点，而是将其编码为潜在空间上的分布。然后模型的训练如下：
首先，输入被编码为潜在空间上的分布 the input is encoded as distribution over the latent space 第二，从该分布中采样潜在空间中的一个点 a point from the latent space is sampled from that distribution 第三，对采样点进行解码并计算重构误差 the sampled point is decoded and the reconstruction error can be computed 最后，重构误差通过网络反向传播 the reconstruction error is backpropagated through the network 在实践中，编码的分布被选择为正态分布（Gaussian mixture distribution），以便编码器可以被训练以返回描述这些高斯分布的均值和协方差矩阵。输入被编码为具有一定方差而不是单点的分布的原因是，它可以非常自然地表达潜在空间正则化：编码器返回的分布被强制接近标准正态分布。我们将在下一小节中看到，我们以这种方式确保潜在空间的局部和全局正则化（局部是因为方差控制，全局是因为均值控制）。</description>
    </item>
    
    <item>
      <title>Deepfake</title>
      <link>https://hugmymind.github.io/posts/deepfake/</link>
      <pubDate>Mon, 25 Sep 2023 15:55:15 +0800</pubDate>
      
      <guid>https://hugmymind.github.io/posts/deepfake/</guid>
      <description>This is a field I will put myself in for 3 years at lease.
Come on !!!
博客主要用来记录自己学习深度学习的经历和论文阅读总结，包括自己对于学科前沿和科技发展的一些看法和思考。
正如url所言，Hug my mind，然后Hug the future。Thank you!</description>
    </item>
    
    <item>
      <title>My first try of post</title>
      <link>https://hugmymind.github.io/posts/mypost/</link>
      <pubDate>Mon, 25 Sep 2023 14:48:38 +0800</pubDate>
      
      <guid>https://hugmymind.github.io/posts/mypost/</guid>
      <description>&lt;p&gt;This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
